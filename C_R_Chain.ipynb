{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPplOp94OkzhgcfUPNip9fu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamsanthoshg/colabFiles/blob/main/C_R_Chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install langchain langchain_groq langchain_community chromadb huggingface_hub"
      ],
      "metadata": {
        "id": "vNPXxAfw5SEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "EB9KI9yt59ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pRnB54ot6jay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.memory import ConversationSummaryMemory"
      ],
      "metadata": {
        "id": "VKJDryMZ5SBN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîπ Step 1: Set Groq API Key\n",
        "os.environ[\"GROQ_API_KEY\"] = \"myGroqApiKey\"  # Replace with your actual API key"
      ],
      "metadata": {
        "id": "0z83w4eW5R-T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîπ Step 2: Load Hugging Face Embeddings\n",
        "print(\"Loading Hugging Face embeddings...\")\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "print(\"Embedding model loaded successfully.\\n\")"
      ],
      "metadata": {
        "id": "L7oa5tjs5R1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QurBz7n4VmM",
        "outputId": "5bbd02a0-f756-474c-de7a-850901125f0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading document...\n",
            "Document loaded successfully: 1 document(s) found.\n",
            "\n",
            "Splitting document into smaller chunks...\n",
            "Document splitting completed: 10 chunks created.\n",
            "\n",
            "Creating vector database with ChromaDB...\n",
            "Vector database created and persisted successfully.\n",
            "\n",
            "Loading Mistral model from Groq API...\n",
            "Mistral model loaded successfully.\n",
            "\n",
            "Initializing conversational memory...\n",
            "Memory initialized successfully.\n",
            "\n",
            "Initializing Conversational Retrieval Chain...\n",
            "Conversational Retrieval Chain initialized successfully.\n",
            "\n",
            "\n",
            "===== Chatbot Ready! Start asking questions =====\n",
            "\n",
            "You: hi\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "We have borne with their present government through all these bitter months because of that friendship‚Äîexercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the\n",
            "\n",
            "We have borne with their present government through all these bitter months because of that friendship‚Äîexercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the\n",
            "Human: hi\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "ü§ñ Bot: Hi! It seems like you started reading a passage about friendship and patience. If you have a question about this passage or its context, feel free to ask, and I'll do my best to help!\n",
            "\n",
            "üìù Chat History: [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi! It seems like you started reading a passage about friendship and patience. If you have a question about this passage or its context, feel free to ask, and I'll do my best to help!\", additional_kwargs={}, response_metadata={})]\n",
            "\n",
            "You: quit\n",
            "Exiting chatbot. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# üîπ Step 3: Load and Process Documents\n",
        "print(\"Loading document...\")\n",
        "loader = TextLoader(\"speech.txt\")  # Make sure speech.txt is in the same directory\n",
        "documents = loader.load()\n",
        "print(f\"Document loaded successfully: {len(documents)} document(s) found.\\n\")\n",
        "\n",
        "# üîπ Step 4: Split Documents into Chunks\n",
        "print(\"Splitting document into smaller chunks...\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "print(f\"Document splitting completed: {len(docs)} chunks created.\\n\")\n",
        "\n",
        "# üîπ Step 5: Create Vector Database (Chroma)\n",
        "print(\"Creating vector database with ChromaDB...\")\n",
        "vector_db = Chroma.from_documents(docs, embedding=embedding_model, persist_directory=\"./chroma_db\")\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})  # Retrieve top 5 matches\n",
        "print(\"Vector database created and persisted successfully.\\n\")\n",
        "\n",
        "# üîπ Step 6: Load LLM (Groq API)\n",
        "print(\"Loading Mistral model from Groq API...\")\n",
        "llm = ChatGroq(model_name=\"llama3-8b-8192\", groq_api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "print(\"Mistral model loaded successfully.\\n\")\n",
        "\n",
        "# üîπ Step 7: Add Conversational Memory\n",
        "print(\"Initializing conversational memory...\")\n",
        "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "print(\"Memory initialized successfully.\\n\")\n",
        "\n",
        "# üîπ Step 8: Create Conversational Retrieval Chain\n",
        "print(\"Initializing Conversational Retrieval Chain...\")\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    output_key=\"answer\",\n",
        "    verbose=True\n",
        ")\n",
        "print(\"Conversational Retrieval Chain initialized successfully.\\n\")\n",
        "\n",
        "# üîπ Step 9: Run Interactive Chat Loop\n",
        "print(\"\\n===== Chatbot Ready! Start asking questions =====\")\n",
        "while True:\n",
        "    query = input(\"\\nYou: \")\n",
        "\n",
        "    if query.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
        "        print(\"Exiting chatbot. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    response = conversation_chain.invoke({\"question\": query})\n",
        "\n",
        "    # Display Answer\n",
        "    print(\"\\nü§ñ Bot:\", response[\"answer\"])\n",
        "\n",
        "    chat_history = memory.load_memory_variables({})\n",
        "    print(\"\\nüìù Chat History:\", chat_history[\"chat_history\"])\n"
      ]
    }
  ]
}
